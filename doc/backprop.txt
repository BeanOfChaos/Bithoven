Si on considère un seul neurone.
    1- l'erreur commise entre le résultat d'un neurone (output, noté o) et le résultat qu'il aurait dû donner (training value, notée t) est exprimée par E=(t-o)^2*1/2
    2- l'output d'un neurone est calculé par la fonction de transition (aka fct d'activation) ça peut être potentiellement n'importe quoi, mais souvent la fct logistique est utilisée : a(x) = 1/(1+e^-x). Sa dérivée est da(x)/dx = a(x)(1-a(x))
    3- dans a(x) la valeur de x est l'input du neurone (noté i). l'input d'un neurone Nk est la somme des produits entre l'output de chaque neurone Nj de la couche précédente et le poid donné à l'arc entre Nk et Nj : i = sig_{k=1 -> n} w_jk*o_j (où w_jk est le poid de l'arc entre Nj et Nk et o_j est l'output de NJ).
    4- nous cherchons à minimiser l'erreur sur l'output o. Pour le faire, nous dérivons partiellement l'erreur E en fonction du poid des arcs entre chaque neurone Nk et Nj :
            dE_k/dw_jk = dE_k/do_k * do_k/di_k * di_k/dw_jk
            di_k/dw_jk = ... = o_j
            do_k/di_k = da(i_k)/di_k = a(i_k) = 1/(1+e^-i_k)     (à condition que a soit une fct logistique)
            pour dE_k/do_k, dans le cas où Nk est dans la couche output, dE_k/do_k = d((t_k-o_k)^2*1/2) / do_k = t_k-o_k
                dans les autres cas, le résultat de cette dérivée dépend des neurones de la couche suivante. Si le set de neurones de la couche suivante est noté L :
                dE_k/do_k = sig_{l in L} (dE_k/di_l * di_l/do_l) = sig_{l in L} (dE_k/do_l * do_l/di_l * w_kl)

            => dE_k/dw_jk = b_k*o_j où b_k = dE_k/do_k * do_k/di_k = | (o_k-t_k)*o_k*(1-o_k)    si Nk est dans la couche output
                                                                     | (sig_{l in L} (w_kl*b_l)*o_k*(1-o_k))
    5- une fois la dérivée calculée, on peut mettre à jour les poids (c'est le processus d'apprentissage). On aura fixé un "learning rate" R positif non-nul. La quantité -R*b_k*o_j est ajoutée au poid w_jk. cette mise à jour a pour but de minimiser l'erreur.

    Note : Relu est une fonction d'activation qu'on utilise à la place de la fonction logistique : a(x) = max(0,x)
        l'utiliser donnerait de meilleurs résultats, askip, mais ça veut dire refaire les formules qui impliquent des dérivées ci-dessus dans le cas d'une implémentation d'un CNN "à la main"


